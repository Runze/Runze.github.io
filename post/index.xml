<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on What I talk about when I don&#39;t talk about me</title>
    <link>/post/</link>
    <description>Recent content in Posts on What I talk about when I don&#39;t talk about me</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 31 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>It’s the end of the year, let me talk about myself.</title>
      <link>/2018/12/31/it-s-the-end-of-the-year-let-me-talk-about-myself/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/31/it-s-the-end-of-the-year-let-me-talk-about-myself/</guid>
      <description>The title says it all.
No, seriously, this post is all about me. So much for not talking about me&amp;hellip;
This may be cliché but, on this very last day of 2018, I want to end the year with a reflection of things that have greatly influenced me this year. These “things” come in various shapes and forms: persons, books, podcasts, courses, events, and YouTube series. Whatever they are, as long as they left a big impression on me and helped me grow, be it personally or professionally, I’m going to list them here.</description>
    </item>
    
    <item>
      <title>Do I really need attention in my seq2seq model?</title>
      <link>/2018/12/19/do-i-really-need-attention/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/19/do-i-really-need-attention/</guid>
      <description>Background Since the origin of the idea of attention (Bahdanau et al., 2015), it has become a norm to try to insert it in a seq2seq model, especially in translations. It is such an intuitive and powerful idea (not to mention the added benefit of peaking into an otherwise blackbox model) that many tutorials and blog posts made it sound like one should not even bother with a model without it as the results would for sure be inferior.</description>
    </item>
    
    <item>
      <title>Second Language Acquisition Modeling using Duolingo&#39;s Data</title>
      <link>/2018/11/26/second-language-acquisition-modeling/</link>
      <pubDate>Mon, 26 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/26/second-language-acquisition-modeling/</guid>
      <description>Background Recently, I have discovered the Second Language Acquisition Modeling (SLAM) challenge hosted by Duolingo earlier this year, where they had asked the participants to predict the per-token error rate for a given language learner based on his/her past learning history. To conclude the competition, the team has also written a paper to summarize the results and the approaches that were taken by the various participants and their respective effectiveness.</description>
    </item>
    
    <item>
      <title>Les lobbys aux Etats-Unis</title>
      <link>/2018/10/03/les-lobbys-aux-etats-unis/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/03/les-lobbys-aux-etats-unis/</guid>
      <description>En théorie (du moins d’après les lobbys eux-mêmes), le gouvernement a besoin des lobbys parce qu’il faut avoir toutes les informations et toutes les opinions avant de prendre une décision. Donc, avoir des lobbys indépendants qui représentent les intérêts de nombreuses parties est une condition fondamentale pour la démocratie. Le problème, pourtant, c’est en réalité les lobbys ne sont ni indépendants ni représentatifs des citoyens.
La National Rifle Association (NRA), l’un des plus grands et plus puissants lobbys aux Etats-Unis, illustre clairement comment ce système fonctionne en réalité.</description>
    </item>
    
    <item>
      <title>Le projet de crédit social en Chine</title>
      <link>/2018/09/21/le-projet-de-cr%C3%A9dit-social-en-chine/</link>
      <pubDate>Fri, 21 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/21/le-projet-de-cr%C3%A9dit-social-en-chine/</guid>
      <description>Quand j’ai entendu que le gouvernement chinois avait annoncé un système de crédit social, j’ai eu une impression de déjà-vu. Enfin, ce ne sera ni la première ni la dernière fois que notre gouvernement essaie de réguler la société par le biais d’une surveillance centrale et mutuelle. Maintenant, le gouvernement a énormément d’expérience quand il s’agit d’unir les citoyens et de les convaincre de travailler pour n’importe quel objectif central même si ça leur demande de se tourner le dos.</description>
    </item>
    
    <item>
      <title>L’assimilation vs. le multiculturalisme</title>
      <link>/2018/09/09/l-assimilation-vs-le-multiculturalisme/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/09/l-assimilation-vs-le-multiculturalisme/</guid>
      <description>Récemment, inspiré par l’actualité, j’ai beaucoup réfléchi sur le sujet de l’immigration, en particulier, sur les différences entre le modèle de l’assimilation, comme celui adopté par France, et le modèle de multiculturalisme, comme celui adopté par les Etats-Unis. En gros, l’assimilation demande que les immigrés s’intègrent à la société du pays d’accueil rapidement et discrètement alors que le multiculturalisme demande que les autochtones tolèrent les différentes cultures et accueillent les changements.</description>
    </item>
    
    <item>
      <title>La joie d’être étranger</title>
      <link>/2018/05/06/la-joie-d-%C3%AAtre-%C3%A9tranger/</link>
      <pubDate>Sun, 06 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/06/la-joie-d-%C3%AAtre-%C3%A9tranger/</guid>
      <description>Dès que j’était petit, je rêvais de voyager à l’étranger. En fait, quand j’avais cinq ou six ans, mes parents m’ont enregistré en parlant de ce que je voudrais faire quand je sera grand et j’ai dit avec confiance que j’aimerais voir et vivre dans les pays différents, ce que je n’ai jamais oublié.
C’est pour ça que j’aime toujours apprendre les langues étrangères. A l’école, mon cours préféré était l’anglais et j’ai lu beaucoup de littérature étrangère et vu beaucoup de films.</description>
    </item>
    
    <item>
      <title>Syndrome de l’imposteur</title>
      <link>/2018/04/08/syndrome-de-l-imposteur/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/08/syndrome-de-l-imposteur/</guid>
      <description>Ça fait longtemps que j’ai voulu écrire quelque chose sur le syndrome de l’imposteur. Enfin c’est un sujet très personnel pour moi. Oui, je me sens comme un imposteur pendant toute ma carrière en tant que data scientist. Donc, je crois que j’ai beaucoup d’expérience à partager.
Au début, c’était que j’avais peur de ne pas avoir étudié assez de statistiques à l’université, ce qui fait qu’il fallait que j’en apprenne beaucoup sur travail.</description>
    </item>
    
    <item>
      <title>Second attempt at building a language translator</title>
      <link>/2017/09/07/second-attempt-at-building-a-language-translator/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/07/second-attempt-at-building-a-language-translator/</guid>
      <description>Background A few weeks ago, I experimented with building a language translator using a simple sequence-to-sequence model. Since then, I had been itchy to add an extra attention layer to it that I had been reading so much about. After many, many research, I came across (quite accidentally) this MOOC series offered by fast.ai, where on Lesson 13, instructor Jeremy Howard walked the students through a practical implementation of the attention mechanism using PyTorch.</description>
    </item>
    
    <item>
      <title>First attempt at building a language translator</title>
      <link>/2017/08/14/first-attempt-at-building-a-language-translator/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/14/first-attempt-at-building-a-language-translator/</guid>
      <description>Background After having tried my hands on LSTM and built a text generater, I became interested in the sequence-to-sequence models, particularly their applications in language translations. It all started with this TensorFlow tutorial where the authors demonstrated how they built an English-to-French translator using such a model and successfully translated &amp;ldquo;Who is the president of the United States?&amp;rdquo; into French with the correct grammar (&amp;ldquo;Qui est le président des États-Unis?</description>
    </item>
    
    <item>
      <title>When Jane Austen, Oscar Wilde, and F. Scott Fitzgerald walk into a bar</title>
      <link>/2017/07/27/when-jane-austen-oscar-wilde-and-f-scott-fitzgerald-walk-into-a-bar/</link>
      <pubDate>Thu, 27 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/27/when-jane-austen-oscar-wilde-and-f-scott-fitzgerald-walk-into-a-bar/</guid>
      <description>Background Lately I&amp;rsquo;ve been spending a lot of time learning about deep learning, particularly its applications in natural language processing, a field I have been immensely interested in. Before deep learning, my foray into NLP has been mainly about sentiment analysis1 and topic modeling.2 These projects are fun but they are all limited in analyzing an existing corpus of text whereas I&amp;rsquo;m also interested in applications that generate texts themselves.</description>
    </item>
    
    <item>
      <title>An analysis of foreign language learning using Duolingo&#39;s data</title>
      <link>/2017/07/02/an-analysis-of-foreign-language-learning-using-duolingo-data/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/02/an-analysis-of-foreign-language-learning-using-duolingo-data/</guid>
      <description>Background Earlier this year, I decided to learn French, something I&amp;rsquo;ve been thinking about for a long time. Foreign language learning has always been something magical to me: I had a great time learning English when I was at school (my mother tongue is Mandarin Chinese), so much that I would devote all my time to it and ignore my other subjects (not recommended). Hence, when I signed up for a beginner&amp;rsquo;s class in my local Alliance Française and started taking classes regularly, it felt homecoming to me.</description>
    </item>
    
    <item>
      <title>Predicting Yelp ratings using textual reviews</title>
      <link>/2014/12/30/predicting-yelp-ratings-using-textual-reviews/</link>
      <pubDate>Tue, 30 Dec 2014 02:04:42 +0000</pubDate>
      
      <guid>/2014/12/30/predicting-yelp-ratings-using-textual-reviews/</guid>
      <description>Internet is truly full of free and fascinating datasets! I found this Yelp Dataset Challenge the other day that includes, among others, over 1 million reviews (most of which are recent) along with their respective 5-star ratings - excellent text mining material! Although to enter the competition (which ends on 12/31/14), you have to be a current student (which I&amp;rsquo;m not), but everyone is welcome to play around with the data.</description>
    </item>
    
    <item>
      <title>Notes to machine learning</title>
      <link>/2014/11/15/notes-to-machine-learning/</link>
      <pubDate>Sat, 15 Nov 2014 22:21:43 +0000</pubDate>
      
      <guid>/2014/11/15/notes-to-machine-learning/</guid>
      <description>Data transformation Resampling techniques Regression models Smoothing Neural networks Support vector machines K-nearest neighbors Trees Random forests Gradient boosting trees Cubist Measuring performace in classification models Linear classificatoin models Latent Dirichlet allocation   These are the notes I took while reading An Introduction to Statistical Learning and Applied Predictive Modeling. Some of the notes also came from other sources, but the majority of them are from these two books.</description>
    </item>
    
    <item>
      <title>Quora challenge - answer classification</title>
      <link>/2014/11/15/answer_classification/</link>
      <pubDate>Sat, 15 Nov 2014 22:15:22 +0000</pubDate>
      
      <guid>/2014/11/15/answer_classification/</guid>
      <description>Last night I tried my hands on a Quora challenge that classifies user-submitted answers into &amp;lsquo;good&amp;rsquo; and &amp;lsquo;bad.&amp;rsquo; All the information is anonymized, including the variable names, but you can tell by looking at their values what some of them may represent. For example, some appear to be count data or some summary statistics based on them, and, given that many of the values are 0 and heavily right-skewed, they seem to be some measure of the writers&amp;rsquo; reputations, the number of upvotes an answer received, or the follow-up comments.</description>
    </item>
    
    <item>
      <title>Building a book recommender</title>
      <link>/2014/10/04/building-a-book-recommender/</link>
      <pubDate>Sat, 04 Oct 2014 20:53:36 +0000</pubDate>
      
      <guid>/2014/10/04/building-a-book-recommender/</guid>
      <description>Lately, I&amp;rsquo;ve become very interested in text mining and topic modeling, and have played around with some popular algorithms like LDA. However, so far my projects have all been centered around what I can learn from a giant chunk of texts and usually stopped after I extracted some revealing and, if I&amp;rsquo;m lucky, thought-provoking topics from them. In other words, what I&amp;rsquo;ve been doing so far is all inference but no predictions.</description>
    </item>
    
    <item>
      <title>Analyzing 八零后 (China&#39;s post-80s )</title>
      <link>/2014/09/26/analyzing-chinas-post-80s/</link>
      <pubDate>Fri, 26 Sep 2014 17:31:57 +0000</pubDate>
      
      <guid>/2014/09/26/analyzing-chinas-post-80s/</guid>
      <description>If you are not from China or living there, you are probably not familiar with the term 八零后, or post-80s, but if you are, like me, I think you&amp;rsquo;ll agree that this is probably one of the most widely used and abused terms in modern China. Quite literally, it refers to Chinese people who were born in the 1980s (me included) and the reason it gained so much attention and exposure as compared to, say, 九零后 (post-90s) or 零零后 (post-00s), I think, stems from the fact that our generation has simply seen and been through way too many things that have never been seen or experienced by prior generations and are simply taken as norms for later ones.</description>
    </item>
    
    <item>
      <title>What mining my own emails told me about myself</title>
      <link>/2014/09/02/what-mining-my-own-emails-told-me-about-myself/</link>
      <pubDate>Tue, 02 Sep 2014 02:05:07 +0000</pubDate>
      
      <guid>/2014/09/02/what-mining-my-own-emails-told-me-about-myself/</guid>
      <description>On Tuesday last week, I attended a data visualization meetup organized by Data Science LA and the topic was about the most recent Eyeo Festival. Of all the talks that Amelia shared with us, what impressed me and inspired me the most was Nicholas Felton&amp;rsquo;s personal data projects. In case you are not familiar with him, every year he publishes an annual report that documents his personal data projects / experiments conducted throughout the year.</description>
    </item>
    
    <item>
      <title>Random acts of pizza - a Kaggle competition</title>
      <link>/2014/08/19/random-acts-of-pizza/</link>
      <pubDate>Tue, 19 Aug 2014 06:22:28 +0000</pubDate>
      
      <guid>/2014/08/19/random-acts-of-pizza/</guid>
      <description>This weekend, I participated in a Kaggle not-for-prize competition that uses data obtained from Reddit&amp;rsquo;s Random Acts of Pizza forum to analyze and predict the outcome of a request for pizza, and it was heaps of fun (I always wanted to say that)! Compared with other Kaggle competitions I had tried before, I found this one a bit easier because the dataset is not very large (~5,000 records) and is hence perfect for model experimenting, and, more importantly, the competition is based on a real research done by a couple Stanford researchers, which provides me with a lot of guidelines in how to proceed.</description>
    </item>
    
    <item>
      <title>Filtering twitter timeline</title>
      <link>/2014/08/12/filtered-twitter-timeline/</link>
      <pubDate>Tue, 12 Aug 2014 06:49:26 +0000</pubDate>
      
      <guid>/2014/08/12/filtered-twitter-timeline/</guid>
      <description>Ever felt your twitter newsfeed has too much going on that you don&amp;rsquo;t have time to read them all, let alone digest? I certainly do, even though I only follow like 20 people. Whenever I open the app, I was &amp;ldquo;bombarded&amp;rdquo; by all the new tweets and, even after scrolling through all of them (as I feel obligated to), I don&amp;rsquo;t feel I have actually taken any new information in. How nice would it be if someone handpicked and highlighted all the useful information for us?</description>
    </item>
    
    <item>
      <title>Location search using Factual and interactive maps using Leaflet</title>
      <link>/2014/08/01/location-search-using-factual-and-interactive-maps-using-leaflet/</link>
      <pubDate>Fri, 01 Aug 2014 06:27:32 +0000</pubDate>
      
      <guid>/2014/08/01/location-search-using-factual-and-interactive-maps-using-leaflet/</guid>
      <description>Leaflet is a popular javascript library for making interactive maps. Don&amp;rsquo;t know how to code in js? No problem, thanks to Ramnath Vaidyanathan, you can now use rCharts to do it in R! Now that we have R Shiny, it just seems a natural thing to combine the two together to make Shiny apps for interactive maps. If that doesn&amp;rsquo;t motivate you, take a look at these cool examples and roll up your sleeves and make one yourself (example1, example2)!</description>
    </item>
    
    <item>
      <title>State sentiment analysis using twitter live stream and R</title>
      <link>/2014/07/07/state-sentiment-analysis-using-twitter-live-stream-and-r/</link>
      <pubDate>Mon, 07 Jul 2014 03:33:22 +0000</pubDate>
      
      <guid>/2014/07/07/state-sentiment-analysis-using-twitter-live-stream-and-r/</guid>
      <description>This week I started taking this Coursera class called Introduction to Data Science taught by Bill Howe from University of Washington. Although there has only been one lesson so far, my experience has been quite positive particularly due to the interesting programing assignment, which is to use twitter&amp;rsquo;s live stream data to analyze tweet sentiment. If you are interested and want to try yourself, you can read the very helpful instruction here and clone the git here (I believe you can access it without signing up for the class, but the course is free anyway).</description>
    </item>
    
    <item>
      <title>No more finding out about a concert too late</title>
      <link>/2014/07/01/no-more-finding-out-about-a-concert-too-late/</link>
      <pubDate>Tue, 01 Jul 2014 06:40:44 +0000</pubDate>
      
      <guid>/2014/07/01/no-more-finding-out-about-a-concert-too-late/</guid>
      <description>After the numerous times of finding out about a concert too late and ending up either paying a premium or not being able to go, I finally decided to do something about it, and this is what I came up with.
https://runzemc.shinyapps.io/pitchfork/
This R Shiny app I made pulls data from pitchfork automatically every time it&amp;rsquo;s open and shows the upcoming shows per city and per artist (indie artist, to be precise).</description>
    </item>
    
    <item>
      <title>An analysis of the most played artists on KCRW</title>
      <link>/2014/06/22/an-analysis-of-the-most-played-artists-on-kcrw/</link>
      <pubDate>Sun, 22 Jun 2014 04:32:24 +0000</pubDate>
      
      <guid>/2014/06/22/an-analysis-of-the-most-played-artists-on-kcrw/</guid>
      <description>R Shiny is an R package that is designed to easily create and deploy pretty web apps all in the nifty RStudio. Right now, it may not be able to make sophisticated or aesthetically pleasing web apps like d3.js, but, by leveraging R&amp;rsquo;s powerhouse analytical capability, I believe it has great potentials. One possible application I can think of is education. Take this k-means app for instance, I wish I had a chance to play with this interactive app when learning about the algorithm myself.</description>
    </item>
    
    <item>
      <title>An analysis of Artsy’s twitter followers</title>
      <link>/2014/05/07/an-analysis-of-artsy-s-twitter-followers/</link>
      <pubDate>Wed, 07 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/05/07/an-analysis-of-artsy-s-twitter-followers/</guid>
      <description>This weekend I decided to learn more about twitter and its handy API. My subject of the analysis is Artsy, a fine-art website that provides a pandora-like service. The subjects I was curious to find out are where their followers are from, what their twitter activities are like, what other interests they have, and, specifically, what kind of stereotypes clusters they fall into because, you know, it’s important and I didn’t have anything better to do.</description>
    </item>
    
  </channel>
</rss>