<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="generator" content="Hugo 0.20.7" />

  <title>Random acts of pizza - a Kaggle competition &middot; An analysis of random things</title>

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="../../../../css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="../../../../css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="../../../../css/blackburn.css">

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  

  

  <link rel="shortcut icon" href="../../../../img/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  

  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="../../../../">Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="../../../../about/">About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://github.com/Runze">GitHub</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://twitter.com/runze_w">Twitter</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small></small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Random acts of pizza - a Kaggle competition</h1>
  <h2></h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>19 Aug 2014, 06:22</time>
  </div>

  

  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="../../../../tags/kaggle">Kaggle</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="../../../../tags/machine-learning">Machine learning</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="../../../../tags/r">R</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="../../../../tags/text-mining">Text mining</a>
    
  </div>
  
  

</div>

  <p>This weekend, I participated in a Kaggle not-for-prize competition that uses data obtained from Reddit&rsquo;s Random Acts of Pizza <a href="http://www.reddit.com/r/Random_Acts_Of_Pizza/">forum</a> to analyze and predict the outcome of a request for pizza, and it was <em>heaps</em> of fun (I always wanted to say that)! Compared with other Kaggle competitions I had tried before, I found this one a bit easier because the dataset is not very large (~5,000 records) and is hence perfect for model experimenting, and, more importantly, the competition is based on a real <a href="http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf">research</a> done by a couple Stanford researchers, which provides me with a lot of guidelines in how to proceed. As my first shot, following the paper, I replicated their study by constructing the same set of variables, and used them to train a few predictive models. The result, .69 ROC, landed me at <a href="https://www.kaggle.com/c/random-acts-of-pizza/leaderboard">No. 13</a> (out of 110)! I have put all my code on <a href="https://github.com/Runze/pizza">github</a> (including the part related to topic modeling and data exploration) and here is a detailed explanation of what I did.</p>

<p><strong>Topic modeling</strong></p>

<p>Following the authors&rsquo; strategy, the first thing I did was mining the requests (as it turned out, there are a fair amount of posts with blank content. Hence, I concatenated the request content with the request title). After cleaning them up, removing stop words and keeping only nouns (using the <a href="http://cran.r-project.org/web/packages/openNLP/openNLP.pdf">openNLP</a> package), I created a document-term matrix (DTM) using them and removed the very high- and low-frequency words (as they don&rsquo;t help separate topics) using the <a href="http://en.wikipedia.org/wiki/Tf–idf">term frequency–inverse document frequency</a> (tf-idf) metric (per the method and code described in this <a href="http://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf">vignette</a>). Using this trimmed DTM, I applied the <a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet allocation</a> (LDA) algorithm from the topicmodels package (again per the vignette linked above) to identify topics. Note in the paper, the authors used &ldquo;non-negative matrix factorization (NMF)&rdquo; to perform topic modeling. However, I wasn&rsquo;t able to find a package in R to do that. That aside, the 10 buckets of topics I got are not as clearly defined and distinguished as those found by the authors. In fact, I couldn&rsquo;t see any clear-cut difference in the frequent terms used in my topics at all. Curious, I tried to find the optimal number of topics through a 10-fold cross-validation and evaluated the resulting split using perplexity, which, as defined in the vignette, uses the log likelihood and shares an inverse relationship with it (hence, the lower the perplexity the better the model fits). Funnily, the results, as shown below, suggest 10 is indeed the optimal split (in the range of 2 to 10):</p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/perplex1.jpg"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/perplex1-300x296.jpg" alt="perplex" /></a>Perhaps a little hard to see, but 8 out of 10 cross-validation holdout sets picked 10 as the optimal value. Here are the most frequent terms used in the 10 topics identified by my model:</p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda10.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda10-300x300.png" alt="lda10" /></a> <a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda9.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda9-300x300.png" alt="lda9" /></a></p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda8.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda8-300x300.png" alt="lda8" /></a> <a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda7.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda7-300x300.png" alt="lda7" /></a></p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda6.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda6-300x300.png" alt="lda6" /></a> <a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda5.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda5-300x300.png" alt="lda5" /></a></p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda4.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda4-300x300.png" alt="lda4" /></a> <a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda3.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda3-300x300.png" alt="lda3" /></a></p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda2.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda2-300x300.png" alt="lda2" /></a> <a href="http://www.runzemc.com/wp-content/uploads/2014/08/lda1.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/lda1-300x300.png" alt="lda1" /></a>
Although there are some patterns detected such as financial difficulties, family, parties, and so on, they are far less clear than those identified by the paper. Besides the different algorithms used, it may as well be that the authors performed more sophisticated cleaning and mining than I did. Out of curiosity, in addition to topic modeling, I also tried simple spherical k-means which, by default, restricts one document to only one cluster and hence forces the &ldquo;topic&rdquo; separation. As a result, my resulting 5 clusters (not shown here) are more different from each other and the patterns are closer to those found by the authors. Nevertheless, topic modeling was <em>heaps</em> of fun!</p>

<p><strong>Create and explore variables</strong></p>

<p>Following the authors&rsquo; methodology, I created the same set of variables that they used in their logistic regression. With regard to what these variables are, what they represent, and how they are constructed, please refer to the original paper. Here, I&rsquo;ll just show their relationship with the success rate of the pizza request. First, let&rsquo;s look at the 5 narrative buckets created using the key words suggested by the authors. They are created by matching the requests against those key words using regular expressions, counting the frequencies, dividing those by the total length of the request (including the title), and converted them into deciles. Here are the relationships between the deciles and the success rate for each bucket (the 0 deciles shown below represent the requests that do not have any key words for a particular bucket): <a href="http://www.runzemc.com/wp-content/uploads/2014/08/narrative.jpg"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/narrative-1024x1013.jpg" alt="narrative" /></a>Interestingly, with the exception of money and craving, these narrative categories do not exhibit a clear linear relationship with the outcome. One of the reasons is that no other factors that may drive the outcome are controlled yet. At least for those that do show a clear trend, the relationship is in line with what the study has found (e.g., more mentions of money troubles lead to a greater chance of success whereas more mentions of craving lead to the opposite).</p>

<p>The other variables are more straightforward and their outcome is also more intuitive. Hence, I&rsquo;ll just show their relationship with the success rate here. You can find the creation process in my code linked above.</p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/explore1.jpeg"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/explore1.jpeg" alt="explore" /></a><a href="http://www.runzemc.com/wp-content/uploads/2014/08/explore.jpeg">
</a><strong>Training models</strong></p>

<p>Now that we have all the variables, we can finally train some models! The first model I tried is the same logistic regression used by the authors. Even without the paper, I think logistic regression, or linear regression in general, should always be the first thing to try because its inference power is very valuable in understanding the relationship between the explanatory variables and the outcome. Even if prediction is the sole purpose, I think it&rsquo;s necessary, not to mention fun, to understand what the model can tell us about the hidden relationships. Here are the coefficients and their statistical significance I got from the logit model:</p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/Screen-Shot-2014-08-18-at-10.14.32-PM.png"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/Screen-Shot-2014-08-18-at-10.14.32-PM-300x284.png" alt="Screen Shot 2014-08-18 at 10.14.32 PM" /></a></p>

<p>Comparing with the paper, I found my results are generally in line with the authors&rsquo;. Specifically, we both found that gratitude, hyperlink (e.g., image), reciprocity, request length, karma, whether the requestor has posted on RAOP before, and mentions of family and money are all positively statistically significant while a request&rsquo;s community age (i.e., the time passed since its post) and mentions of craving are both negatively statistically significant. Using a 10-fold cross-validation, I got an ROC of .664, which is close to their .669. Win!</p>

<p>In addition to logistic regression, I also tried random forests, gradient boost trees, and neural networks. Their performance, in terms of ROC, is shown in the parallel plot below (each line represents the results for a common cross-validation holdout set):</p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/parallel.jpeg"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/parallel-300x300.jpeg" alt="parallel" /></a></p>

<p>As shown above, the models yield comparable results with gradient boost trees performing slightly better than the other. Besides the performance itself, I think it&rsquo;s also interesting to see how each model ranks the variable importance, so I made these plots as well:</p>

<p><a href="http://www.runzemc.com/wp-content/uploads/2014/08/importance.jpeg"><img src="http://www.runzemc.com/wp-content/uploads/2014/08/importance-300x297.jpeg" alt="importance" /></a>As it turned out, although the 4 models don&rsquo;t agree on everything, they do all consider variables such as whether the requester has posted on the forum before, the post&rsquo;s community age, karma, request length, and mentions of money as those that matter the most, which makes intuitive sense.</p>

<p><strong>Predicting on test set</strong></p>

<p>Finally, we are going to take these models on a test drive! Because of the similar performance, I combined them together by averaging the predicting result (i.e., the probability of success). The resulting ROC of .69, as calculated by Kaggle, is in line with the result I got by applying the same ensemble to the &ldquo;test&rdquo; set carved out from the training set using stratified sampling (by using the createDataPartition() function from the <a href="http://topepo.github.io/caret/index.html">caret</a> package). In fact, they are the same down to the 4th digit!</p>

<p><strong>Conclusion</strong></p>

<p>Similar to what the authors have found, a request (of anything?) is most likely to be granted if enough effort has been put into making them. This may include a detailed description of the situation, an explanation that will likely evoke empathy, and the requestor&rsquo;s own reputation. Armed with this knowledge, I&rsquo;m going to try my luck on the RAOP forum myself.</p>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="../../../../2014/08/12/filtered-twitter-timeline/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="../../../../2014/08/12/filtered-twitter-timeline/">Filtering twitter timeline</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
    <nav class="next">
      <a href="../../../../2014/09/02/what-mining-my-own-emails-told-me-about-myself/">What mining my own emails told me about myself</a>
    </nav>
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
    <a href="../../../../2014/09/02/what-mining-my-own-emails-told-me-about-myself/"><i class="fa fa-chevron-right"></i></a>
    
  </div>
</div>



  

</div>

</div>
</div>
<script src="../../../../js/ui.js"></script>




</body>
</html>

