<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="A website built through Hugo and blogdown.">
  <meta name="generator" content="Hugo 0.20.7" />

  <title>What I talk about when I don&#39;t talk about me</title>

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.2/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="./css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="./css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="./css/blackburn.css">

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  
  <link rel="alternate" type="application/rss+xml" title="What I talk about when I don&#39;t talk about me" href="./index.xml" />
  

  

  <link rel="shortcut icon" href="./img/favicon.ico" type="image/x-icon" />

  
  

</head>


<head>
  <link rel="stylesheet" href="./css/github-gist.css">
  <script src="./js/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>

<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  

  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="./">Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="./technical-posts/">Technical Posts</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="./other-musings/">Other Musings</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="./about/">About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://github.com/Runze">GitHub</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://www.linkedin.com/in/runze-wang/">LinkedIn</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="./index.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small></small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>What I talk about when I don&#39;t talk about me</h1>
  <h2></h2>
</div>

<div class="content">
  
    <article>
  <header>
    <h2><a href="./technical-posts/building-a-story-telling-twitter-bot/">Building a story-telling Twitter bot</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2019-11-13</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/deep-learning">Deep Learning</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/nlp">NLP</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/aws">AWS</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  Background Recently, I came across this blog post written by Vicki Boykis in which she documented her process of building a Twitter bot that tweets Soviet artworks in scheduled intervals. Inspired by the idea (and motivated by boredom), I decided to build a Twitter bot myself that tells questionably coherent stories through a series of tweets. I loved this idea because first of all, I love literature, especially the classics.
  </p>

  
  <footer>
    <a href="./technical-posts/building-a-story-telling-twitter-bot/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="./other-musings/my-month-of-unemployment/">My month of unemployment</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2019-10-14</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/life-story">Life story</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  On September 10, 2019, I lost my job in a mass layoff. Now, after having endured a month-long unemployment, fortunately concluded by signing another job offer today, I decided to sit down and retrospectively document my journey, which, although arguably small and inconsequential in the grand scheme of things, was important and transformative for me. As the cliché goes, you learn a lot about yourself when going through obstacles. Indeed, over the span of this month, I have subsequently come to identify my weakness, my insecurity, my solitude, and later, my resilience and my self-worth.
  </p>

  
  <footer>
    <a href="./other-musings/my-month-of-unemployment/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="./other-musings/intended-obliviousness-and-unintended-consequences/">Intended obliviousness and unintended consequences</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2019-04-22</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/ai">AI</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/society">Society</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  Tonight, I had the chance to attend a talk given by one of my favorite non-fiction writers, Yuval Noah Harari, whose latest book, 21 Lessons for the 21st Century, has quite notably set off a long, enduring existential crisis for me ever since I read it. Organized by Stanford&rsquo;s Human-Centered AI organization, the talk was focused on the societal impact of AI and was carried out in a conversational style with Stanford&rsquo;s own leading AI researcher, Fei-Fei Li.
  </p>

  
  <footer>
    <a href="./other-musings/intended-obliviousness-and-unintended-consequences/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="./other-musings/the-education-system-in-china/">The education system in China</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2019-03-27</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/china">China</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/education">Education</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  Growing up in China, one of my favorite conversational topics is to complain about its education system. For context, I have 16 years of survival experience and, even though it has been more than a decade since I graduated from high school, I still have regular nightmares about failing an important exam or being punished for talking too loud during a class break (both of which have actually happened). Hence, I do consider myself a subject expert.
  </p>

  
  <footer>
    <a href="./other-musings/the-education-system-in-china/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="./other-musings/it-s-the-end-of-the-year-let-me-talk-about-myself/">It’s the end of the year, let me talk about myself.</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2018-12-31</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/life-story">Life story</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  The title says it all.
No, seriously, this post is all about me. So much for not talking about me&hellip;
This may be cliché but, on this very last day of 2018, I want to end the year with a reflection of things that have greatly influenced me this year. These “things” come in various shapes and forms: persons, books, podcasts, courses, events, and YouTube series. Whatever they are, as long as they left a big impression on me and helped me grow, be it personally or professionally, I’m going to list them here.
  </p>

  
  <footer>
    <a href="./other-musings/it-s-the-end-of-the-year-let-me-talk-about-myself/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="./technical-posts/do-i-really-need-attention/">Do I really need attention in my seq2seq model?</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2018-12-19</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/deep-learning">Deep Learning</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/nlp">NLP</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/rnn">RNN</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  Background Since the origin of the idea of attention (Bahdanau et al., 2015), it has become a norm to try to insert it in a seq2seq model, especially in translations. It is such an intuitive and powerful idea (not to mention the added benefit of peaking into an otherwise blackbox model) that many tutorials and blog posts made it sound like one should not even bother with a model without it as the results would for sure be inferior.
  </p>

  
  <footer>
    <a href="./technical-posts/do-i-really-need-attention/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="./technical-posts/second-language-acquisition-modeling/">Second Language Acquisition Modeling using Duolingo&#39;s Data</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2018-11-26</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/deep-learning">Deep Learning</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/nlp">NLP</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/rnn">RNN</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  Background Recently, I have discovered the Second Language Acquisition Modeling (SLAM) challenge hosted by Duolingo earlier this year, where they had asked the participants to predict the per-token error rate for a given language learner based on his/her past learning history. To conclude the competition, the team has also written a paper to summarize the results and the approaches that were taken by the various participants and their respective effectiveness.
  </p>

  
  <footer>
    <a href="./technical-posts/second-language-acquisition-modeling/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="./technical-posts/second-attempt-at-building-a-language-translator/">Second attempt at building a language translator</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2017-09-07</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/deep-learning">Deep Learning</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/rnn">RNN</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/lstm">LSTM</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/nlp">NLP</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  Background A few weeks ago, I experimented with building a language translator using a simple sequence-to-sequence model. Since then, I had been itchy to add an extra attention layer to it that I had been reading so much about. After many, many research, I came across (quite accidentally) this MOOC series offered by fast.ai, where on Lesson 13, instructor Jeremy Howard walked the students through a practical implementation of the attention mechanism using PyTorch.
  </p>

  
  <footer>
    <a href="./technical-posts/second-attempt-at-building-a-language-translator/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="./technical-posts/first-attempt-at-building-a-language-translator/">First attempt at building a language translator</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2017-08-14</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/deep-learning">Deep Learning</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/rnn">RNN</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/lstm">LSTM</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/nlp">NLP</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  Background After having tried my hands on LSTM and built a text generater, I became interested in the sequence-to-sequence models, particularly their applications in language translations. It all started with this TensorFlow tutorial where the authors demonstrated how they built an English-to-French translator using such a model and successfully translated &ldquo;Who is the president of the United States?&rdquo; into French with the correct grammar (&ldquo;Qui est le président des États-Unis?
  </p>

  
  <footer>
    <a href="./technical-posts/first-attempt-at-building-a-language-translator/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="./technical-posts/when-jane-austen-oscar-wilde-and-f-scott-fitzgerald-walk-into-a-bar/">When Jane Austen, Oscar Wilde, and F. Scott Fitzgerald walk into a bar</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2017-07-27</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="./tags/deep-learning">Deep Learning</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/rnn">RNN</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/lstm">LSTM</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="./tags/nlp">NLP</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  Background Lately I&rsquo;ve been spending a lot of time learning about deep learning, particularly its applications in natural language processing, a field I have been immensely interested in. Before deep learning, my foray into NLP has been mainly about sentiment analysis1 and topic modeling.2 These projects are fun but they are all limited in analyzing an existing corpus of text whereas I&rsquo;m also interested in applications that generate texts themselves.
  </p>

  
  <footer>
    <a href="./technical-posts/when-jane-austen-oscar-wilde-and-f-scott-fitzgerald-walk-into-a-bar/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  

  


<nav class="pagination" role="pagination">
  
  <i class="fa fa-chevron-left"></i>
  
  <span>&nbsp;1 / 3&nbsp;</span>
  
  <a href="./page/2/"><i class="fa fa-chevron-right"></i></a>
  
</nav>



</div>

</div>
</div>
<script src="./js/ui.js"></script>
<script src="//yihui.name/js/math-code.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>




</body>
</html>
